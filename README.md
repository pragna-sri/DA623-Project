# DA623-Project
# VisualBERT: Joint Vision and Language Representation  
*DA623 Course Project â€” Winter 2025*  
Rayala Naga Pragna Sri

---

## ğŸ¯ Overview

This repository presents a concise study and presentation of **VisualBERT**, a pioneering model that integrates vision and language by feeding both image region features and text tokens into a single BERT-style Transformer. VisualBERT demonstrates that a simple early-fusion architecture, trained with masked language modeling and imageâ€“text matching, can rival more complex dual-stream models on a variety of multimodal tasks.

---

## ğŸ“½ï¸ Presentation
**Includes:**  
- Motivation behind unifying visual and textual inputs  
- Architecture and embedding strategy  
- Pre-training objectives  
- Results on VQA, VCR, NLVR2, Flickr30K  
- Attention visualization examples  
- Strengths, limitations, and future directions  

---

## ğŸ““ Blog-Style Notebook

The notebook contains:  
- Motivation and context in multimodal learning  
- Architecture breakdown with annotated diagrams  
- Code snippet demo for attention simulation  
- Visual explanation of attention over image regions  
- Reflections and takeaways  

## ğŸ¥ Presentation Video 

Watch my recorded walkthrough here:  
**â–¶ï¸ [YouTube Video Link](https://youtu.be/PzHU8Kjwv-k)**



