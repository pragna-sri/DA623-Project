# DA623-Project
# VisualBERT: Joint Vision and Language Representation  
*DA623 Course Project — Winter 2025*  
Rayala Naga Pragna Sri

---

## 🎯 Overview

This repository presents a concise study and presentation of **VisualBERT**, a pioneering model that integrates vision and language by feeding both image region features and text tokens into a single BERT-style Transformer. VisualBERT demonstrates that a simple early-fusion architecture, trained with masked language modeling and image–text matching, can rival more complex dual-stream models on a variety of multimodal tasks.

---

## 📽️ Presentation
**Includes:**  
- Motivation behind unifying visual and textual inputs  
- Architecture and embedding strategy  
- Pre-training objectives  
- Results on VQA, VCR, NLVR2, Flickr30K  
- Attention visualization examples  
- Strengths, limitations, and future directions  

---

## 📓 Blog-Style Notebook

The notebook contains:  
- Motivation and context in multimodal learning  
- Architecture breakdown with annotated diagrams  
- Code snippet demo for attention simulation  
- Visual explanation of attention over image regions  
- Reflections and takeaways  

## 🎥 Presentation Video 

Watch my recorded walkthrough here:  
**▶️ [YouTube Video Link](https://youtu.be/PzHU8Kjwv-k)**



